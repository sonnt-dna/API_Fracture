{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbc6064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fracture(full_df, parameter):\n",
    "    #import libraries\n",
    "    import sys\n",
    "    sys.path.append('/lakehouse/default/Files')\n",
    "    sys.path.append('../')\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import xgboost as xgb\n",
    "    import warnings\n",
    "\n",
    "    \"\"\"\" NOTE: Import packages which customized by VPI (therefore can't be installed with \"pip\") \"\"\"\n",
    "    from Devtools.LightGBM._ligthgbmR import Train_LGBM\n",
    "    from Devtools.LightGBM.score_cal import RScore\n",
    "    from Devtools.XGBoost._xgboostR import Train_XGBR\n",
    "    from Devtools.XGBoost.score_cal import RScore\n",
    "\n",
    "    pd.set_option('display.max_columns', 100)\n",
    "    pd.set_option('use_inf_as_na',True)\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    import joblib\n",
    "    from datetime import datetime\n",
    "\n",
    "    seed = 42\n",
    "    df = full_df\n",
    "    print(df)\n",
    "    col = list(df.columns)\n",
    "    if 'DEPT' in col:\n",
    "        df['DEPTH']=df['DEPT'].copy()\n",
    "        df = df.drop(['DEPT'], axis=1)\n",
    "    target_list = ['NPHI', 'RHOB', 'DTS', 'DTC']\n",
    "    scoring_list = ['R2', 'MAE', 'MSE', 'RMSE', 'MAPE', 'Poisson', 'Tweedie', 'MeAE', 'ExVS', 'MSLE', 'ME', 'Gamma', 'D2T', 'D2Pi', 'D2A']\n",
    "    obj_list =['valid_score', 'train_valid_drop']\n",
    "    algorithm_list = [\"lightgbm\", \"xgboost\", \"catboost\"]\n",
    "    well_list = list(df['WELL'].unique())\n",
    "    if len(well_list) !=1:\n",
    "        well_list.append('all data')\n",
    "    else:\n",
    "        well_list = well_list\n",
    "\n",
    "    # ## 1. Exploratory Data Analysis\n",
    "\n",
    "    # In[4]:\n",
    "    well_view = 'all data'\n",
    "    # ### 1.1. Curve missing percentage\n",
    "    if well_view=='all data':\n",
    "        data_view=df\n",
    "    else:\n",
    "        data_view = df.loc[df['WELL'].astype(str) == well_view]\n",
    "        #replace -999 in dataframe\n",
    "    def replace_999(df,col):\n",
    "        df[col]=df[col].replace(-999, np.nan)\n",
    "        return df\n",
    "\n",
    "    col = [col for col in data_view.columns if col not in ['WELL', 'DEPTH']]\n",
    "    replace_999(data_view, col)\n",
    "\n",
    "\n",
    "    # ## 2. Missing log model\n",
    "\n",
    "    # You can choose wells from list below. In case choosing all well, please set well =['all data']\n",
    "\n",
    "\n",
    "    well = ['01-97-HXS-1X','15-1-SN-1X', '15-1-SN-2X','15-1-SN-4X', '15-1-SNN-1P', '15-1-SNN-2P','15-1-SNN-3P','15-1-SNN-4P','15-1-SNS-7P','15-1-SNS-4P','15-1-SNS-2P']\n",
    "\n",
    "\n",
    "    # ### 2.1. Preprocessing\n",
    "\n",
    "    # If you choose single well, you can choose depth interval for training. Otherwise, please type 'none'.\n",
    "    data = df\n",
    "    if len(well)!=1:\n",
    "        print(\"Please type 'none' in from_training and to_training\")\n",
    "    else:\n",
    "        data = data.sort_values(by=['DEPTH'])\n",
    "        print('Min dept:',data['DEPTH'].min())\n",
    "        print('Max dept:', data['DEPTH'].max())\n",
    "\n",
    "\n",
    "    target = 'DTC'\n",
    "    good_data = 'True'\n",
    "    upper_interval = '2.5'\n",
    "    lower_interval = '1.5'\n",
    "    from_training = 'none'\n",
    "    to_training = 'none'\n",
    "\n",
    "    if from_training == 'none':\n",
    "        data=data\n",
    "    else:\n",
    "        data= data.loc[(data['DEPTH'] <= float(to_training))&(data['DEPTH'] >= float(from_training))]\n",
    "    #replace -999 in dataframe\n",
    "    def replace_999(df,col):\n",
    "        df[col]=df[col].replace(-999, np.nan)\n",
    "        return df\n",
    "    #replace negative in columns\n",
    "    def repl_negative(df,col):\n",
    "        df[col]= np.where(df[col] <0,np.nan, df[col])\n",
    "        return df\n",
    "\n",
    "    col = [col for col in data.columns if col not in ['WELL', 'DEPTH']]\n",
    "    replace_999(data, col)\n",
    "    if 'BS' in col:\n",
    "        data['DCALI_FINAL'] = np.where(data['CALI'].isnull(), np.nan, (data['CALI']-data['BS']))\n",
    "    else:\n",
    "        data['DCALI_FINAL'] = data['DCALI_FINAL']\n",
    "\n",
    "    check_negative = ['RHOB', 'LLD', 'LLS', 'DTC', 'DTS']\n",
    "\n",
    "    repl_negative(data, check_negative)\n",
    "\n",
    "    if good_data == 'True':\n",
    "        data = data.loc[(data['DCALI_FINAL'] <= float(upper_interval))&(data['DCALI_FINAL'] >= float(lower_interval))]\n",
    "    else:\n",
    "        data=data\n",
    "\n",
    "\n",
    "    feature_list = [col for col in data.columns if col not in [target, 'WELL']]\n",
    "\n",
    "\n",
    "    print('Done processing!')\n",
    "    print('You can choose features in this list:',feature_list)\n",
    "\n",
    "\n",
    "    # In[25]:\n",
    "\n",
    "\n",
    "    # feature= ['NPHI', 'RHOB', 'DTS']\n",
    "\n",
    "\n",
    "    feature = parameter.get(\"feature\")\n",
    "    # feature = features\n",
    "\n",
    "\n",
    "    # ### 2.2 Model building\n",
    "\n",
    "    scoring = parameter.get(\"scoring\")\n",
    "\n",
    "    objective = parameter.get(\"objective\")\n",
    "\n",
    "    algorithm = parameter.get(\"algorithm\") #algorithm #'xgboost' #'catboost', #'xgboost' #'lightgbm', 'catboost'\n",
    "\n",
    "    show_shap = parameter.get(\"show_shap\")\n",
    "\n",
    "    iteration = parameter.get(\"iteration\")\n",
    "\n",
    "\n",
    "    # scoring = scoring\n",
    "    #\n",
    "    # objective = objective\n",
    "    #\n",
    "    # algorithm = algorithm  # algorithm #'xgboost' #'catboost', #'xgboost' #'lightgbm', 'catboost'\n",
    "    #\n",
    "    # show_shap = show_shap\n",
    "    #\n",
    "    # iteration = iteration\n",
    "    #save parameters to parameter file\n",
    "    section2new = {'target': target,'good_data': good_data,'upper_interval': upper_interval, 'lower_interval': lower_interval, 'scoring': scoring, 'objective': objective, 'algorithm': algorithm, 'show_shap': show_shap,'iteration': iteration}\n",
    "    #check for update\n",
    "    # if section2!=section2new:\n",
    "    #     section2.update(section2new)\n",
    "    #     with open('parameter_section_2.py', 'w') as f:\n",
    "    #         f.write('section2 = ' + str(section2) + '\\n')\n",
    "\n",
    "    drop = feature.copy()\n",
    "    drop.append(target)\n",
    "    drop\n",
    "    data = data.dropna(how ='any', subset=drop)\n",
    "    if objective == 'valid_score':\n",
    "        objective = 0\n",
    "    else:\n",
    "        objective = 1\n",
    "    if target in check_negative:\n",
    "        if algorithm == 'xgboost':\n",
    "            task = 'reg:gamma'\n",
    "        elif algorithm == 'lightgbm':\n",
    "            task = 'gamma'\n",
    "        else:\n",
    "            task ='MAE'\n",
    "    else:\n",
    "        if algorithm == 'xgboost':\n",
    "            task = 'reg:squarederror'\n",
    "        elif algorithm == 'lightgbm':\n",
    "            task = 'regression'\n",
    "        else:\n",
    "            task = 'RMSE'\n",
    "    print(data)\n",
    "    y=data[target]\n",
    "    X=data[feature]\n",
    "    #print(X.isna().sum())\n",
    "    #print(feature)\n",
    "    #split data into sets\n",
    "    X_use, X_test, y_use, y_test = train_test_split(X, y, train_size=0.9, random_state=seed, shuffle=True)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_use, y_use, train_size=0.8, random_state=seed, shuffle=True)\n",
    "    preprocessors = Pipeline(steps=\n",
    "            [\n",
    "                (\"imputer\", SimpleImputer(strategy='most_frequent')),\n",
    "                (\"scaling\", MinMaxScaler())\n",
    "        ]\n",
    "    )\n",
    "    #print(X_train.shape)\n",
    "    X_train, X_valid = preprocessors.fit_transform(X_train), preprocessors.transform(X_valid)\n",
    "    X_test = preprocessors.transform(X_test)\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "    #print(X_train.shape)\n",
    "    #print(X.shape)\n",
    "    #print(feature)\n",
    "    X_train = pd.DataFrame(X_train, columns=feature)\n",
    "    X_valid = pd.DataFrame(X_valid, columns=feature)\n",
    "    X_test  = pd.DataFrame(X_test, columns=feature)\n",
    "\n",
    "    if algorithm=='lightgbm':\n",
    "        model = Train_LGBM(\n",
    "            features = X_train,\n",
    "            target = y_train,\n",
    "            iterations = int(iteration),\n",
    "            scoring = scoring,\n",
    "            validation_size = 0.1,\n",
    "            task = task,\n",
    "            )\n",
    "        y_pred = model.predict(X_test)\n",
    "        score = RScore(y_true=y_test, y_pred=y_pred, scoring=scoring)\n",
    "        print(score)\n",
    "        # model_best = model[\"lightgbm\"]\n",
    "    elif algorithm =='xgboost':\n",
    "        model = Train_XGBR(\n",
    "            features = X_train,\n",
    "            target = y_train,\n",
    "            iterations = int(iteration),\n",
    "            scoring = scoring,\n",
    "            validation_size = 0.1,\n",
    "            # base_score=0.5,\n",
    "            # test_set= (X_test, y_test),\n",
    "            task = task,\n",
    "            # objectives = objective,\n",
    "            # show_shap = show_shap,\n",
    "            # refit = False,\n",
    "            # saved_dir='xgboost_shaps'\n",
    "            )\n",
    "        # model_best = model[\"xgboost\"]\n",
    "        y_pred=model.predict(xgb.DMatrix(data=X_test, label=y_test))\n",
    "        score = RScore(y_true=y_test, y_pred=y_pred, scoring=scoring)\n",
    "        score_Train = RScore(y_true=y_test, y_pred=y_pred, scoring=scoring)\n",
    "        print(f\"Test score: {score}\")\n",
    "    else:\n",
    "        model= Train_CATR(\n",
    "            features=X_train,\n",
    "            target=y_train,\n",
    "            iterations = int(iteration),\n",
    "            base_score=0.5,\n",
    "            scoring=scoring,\n",
    "            validation_size = 0.1,\n",
    "            # test_set=(X_test, y_test),\n",
    "            task=task,\n",
    "            # objectives=objective, #{0: \"valid_score\", 1: \"train_valid_drop\"}\n",
    "            # show_shap=show_shap, # flag to show shap True or False\n",
    "            # refit=False,\n",
    "            )\n",
    "        y_pred=model.predict(cat.Pool(data=X_test, label=y_test))\n",
    "        score = RScore(y_true=y_test, y_pred=y_pred, scoring=scoring)\n",
    "        score_Train = RScore(y_true=y_test, y_pred=y_pred, scoring=scoring)\n",
    "        print(f\"Test score: {score}\")\n",
    "\n",
    "    if algorithm =='xgboost':\n",
    "        model.save_model('/lakehouse/default/Files/Saved_Models/model_json_Goal2.json')\n",
    "        import json\n",
    "        with open('/lakehouse/default/Files/Saved_Models/model_json_Goal2.json') as f:\n",
    "            data = json.load(f)\n",
    "        print(data)\n",
    "        # Convert the model output to a JSON string\n",
    "        model_json_str = json.dumps(data)\n",
    "\n",
    "    elif algorithm=='lightgbm':\n",
    "        import json\n",
    "        model_json = model.dump_model()\n",
    "        # Convert the model output to a JSON string\n",
    "        model_json_str = json.dumps(model_json)\n",
    "    #\n",
    "    else:\n",
    "        model.save_model('/lakehouse/default/Files/Saved_Models/2model_json.json')\n",
    "        import json\n",
    "        with open('/lakehouse/default/Files/Saved_Models/2model_json.json') as f:\n",
    "            data = json.load(f)\n",
    "        print(data)\n",
    "        #Convert the model output to a JSON string\n",
    "        model_json_str = json.dumps(data)\n",
    "\n",
    "    if algorithm=='lightgbm':\n",
    "        full_predict = model.predict(data=X)\n",
    "        arr = np.array(full_predict)\n",
    "        json_str = json.dumps(arr.tolist())\n",
    "    else:\n",
    "        full_predict = model.predict(xgb.DMatrix(data=X, label=y))\n",
    "        dataset_full = full_predict\n",
    "\n",
    "        arr = np.array(dataset_full)\n",
    "        json_str = json.dumps(arr.tolist())\n",
    "    # result_in_json_1 = json_str.to_json(orient='index')\n",
    "#API return for output\n",
    "    custom_result = {}\n",
    "\n",
    "    if parameter.get(\"Testing_score\"):\n",
    "        custom_result[\"Testing_score\"] = score\n",
    "\n",
    "    if parameter.get(\"Train_score\"):\n",
    "        custom_result[\"Train_score\"] = score_Train\n",
    "\n",
    "    if parameter.get(\"Modeling_result\"):\n",
    "        custom_result[\"Modeling_result\"] = model_json_str\n",
    "\n",
    "    if parameter.get(\"Predicted_Results\"):\n",
    "        custom_result[\"Predicted_Results\"] = json_str\n",
    "\n",
    "    result_in_json_full = {\n",
    "        # \"Modeling_result\": model_json_str,\n",
    "        # \"Predicted_Results\": json_str,\n",
    "        **parameter,\n",
    "        **custom_result\n",
    "    }\n",
    "    import json\n",
    "    result_in_json = json.dumps(result_in_json_full)\n",
    "    return result_in_json\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
